# robust-socratic
Repository for all IIB project experiments.

Model distillation with mechanistic robustness.

# Michaelmas
Own dataset counterfactual training
- basic1_teacher_train: train, test and save teacher MLP, implementation of Jose Horasâ€™ knowledge distillation Github
- basic1_student_train: train and test student MLP
- utils: contains custom dataset class(es) [1. y binary, x-k-slabs]
- error: custom exception file

# Lent
Image datasets for variety of distillation types
